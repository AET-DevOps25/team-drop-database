#!/usr/bin/env python3
"""
æ™¯ç‚¹é—®ç­”ç³»ç»Ÿï¼ˆä¿®æ­£ç‰ˆï¼‰
ç›´æ¥é€‚é…ç°æœ‰çš„attractions_collectionæ•°æ®æ ¼å¼
æ”¯æŒå¤šç§LLMæ¨¡å‹ï¼ˆOpenAI, GPT4All, LLaMAç­‰ï¼‰
"""

import asyncio
from typing import List, Dict, Any
from langchain_openai import OpenAIEmbeddings

from travel_buddy_ai.core.config import settings
from travel_buddy_ai.core.db import get_qdrant_connection
from travel_buddy_ai.core.logger import get_logger
from travel_buddy_ai.models.llm_models import model_manager, ModelType

logger = get_logger(__name__)


class AttractionQASystem:
    """æ™¯ç‚¹é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, model_type: str = None):
        self.qdrant_client = get_qdrant_connection()
        self.collection_name = "attractions_collection"
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large", 
            api_key=settings.openai_api_key
        )
        
        # è®¾ç½®LLMæ¨¡å‹
        if model_type:
            if not model_manager.set_model(model_type):
                logger.warning(f"æ¨¡å‹ {model_type} ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨æ¨¡å‹
        if not model_manager.get_current_model():
            raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„LLMæ¨¡å‹ï¼Œè¯·æ£€æŸ¥é…ç½®")
        
        current_model = model_manager.get_current_model()
        logger.info(f"ä½¿ç”¨LLMæ¨¡å‹: {current_model.model_name}")
        logger.info(f"å¯ç”¨æ¨¡å‹åˆ—è¡¨: {model_manager.list_available_models()}")
    
    def switch_model(self, model_type: str) -> bool:
        """
        åˆ‡æ¢LLMæ¨¡å‹
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ (openai, gpt4all, llamacpp, ollama)
            
        Returns:
            æ˜¯å¦åˆ‡æ¢æˆåŠŸ
        """
        success = model_manager.set_model(model_type)
        if success:
            current_model = model_manager.get_current_model()
            logger.info(f"å·²åˆ‡æ¢åˆ°æ¨¡å‹: {current_model.model_name}")
        return success
    
    def list_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return model_manager.list_available_models()
    
    def get_current_model_info(self) -> Dict[str, str]:
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        current_model = model_manager.get_current_model()
        if current_model:
            return {
                "model_name": current_model.model_name,
                "model_type": type(current_model).__name__
            }
        return {"model_name": "None", "model_type": "None"}
    
    def preprocess_query(self, query: str) -> str:
        """
        é¢„å¤„ç†æŸ¥è¯¢ï¼Œå°†ä¸€èˆ¬æ€§é—®é¢˜è½¬æ¢ä¸ºæ›´å…·ä½“çš„æ™¯ç‚¹æœç´¢æŸ¥è¯¢
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            å¤„ç†åçš„æŸ¥è¯¢
        """
        query_lower = query.lower()
        
        # è¡Œç¨‹è§„åˆ’ç±»é—®é¢˜
        if any(keyword in query_lower for keyword in ['è¡Œç¨‹', 'è§„åˆ’', 'å‡ å¤©', 'å®‰æ’', 'è®¡åˆ’', 'itinerary', 'plan']):
            return "æ…•å°¼é»‘æ™¯ç‚¹ æ¨èæ™¯ç‚¹ æ—…æ¸¸æ™¯ç‚¹"
        
        # ç›´æ¥è¿”å›åŸæŸ¥è¯¢
        return query
    
    def search_attractions(self, query: str, limit: int = 8) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸å…³æ™¯ç‚¹
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # é¢„å¤„ç†æŸ¥è¯¢
            processed_query = self.preprocess_query(query)
            logger.info(f"åŸå§‹æŸ¥è¯¢: {query}")
            if processed_query != query:
                logger.info(f"å¤„ç†åæŸ¥è¯¢: {processed_query}")
            
            # å¯¹æŸ¥è¯¢è¿›è¡Œå‘é‡åŒ–
            query_vector = self.embeddings.embed_query(processed_query)
            
            # åœ¨Qdrantä¸­æœç´¢
            search_results = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=("dense", query_vector),  # æŒ‡å®šä½¿ç”¨denseå‘é‡
                limit=limit,
                score_threshold=0.3,  # é™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šç»“æœ
                with_payload=True
            )
            
            results = []
            for result in search_results:
                results.append({
                    "content": result.payload.get("page_content", ""),
                    "metadata": result.payload.get("metadata", {}),
                    "score": result.score
                })
            
            logger.info(f"å‘é‡æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def generate_answer(self, question: str, search_results: List[Dict[str, Any]]) -> str:
        """
        åŸºäºæœç´¢ç»“æœç”Ÿæˆå›ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            search_results: å‘é‡æœç´¢ç»“æœ
            
        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        if not search_results:
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ™¯ç‚¹ä¿¡æ¯ã€‚"
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for result in search_results:
            content = result.get("content", "")
            score = result.get("score", 0)
            context_parts.append(f"[ç›¸å…³åº¦: {score:.3f}] {content}")
        
        context = "\n\n".join(context_parts)
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""
è¯·åŸºäºä»¥ä¸‹æ™¯ç‚¹ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³æ™¯ç‚¹ä¿¡æ¯:
{context}

è¯·æ³¨æ„:
1. åªä½¿ç”¨æä¾›çš„æ™¯ç‚¹ä¿¡æ¯æ¥å›ç­”
2. å¦‚æœç”¨æˆ·é—®çš„æ˜¯è¡Œç¨‹è§„åˆ’ç±»é—®é¢˜ï¼Œè¯·åŸºäºæä¾›çš„æ™¯ç‚¹ä¿¡æ¯åˆ¶å®šåˆç†çš„è¡Œç¨‹å®‰æ’
3. å›ç­”è¦è¯¦ç»†ã€æœ‰ç”¨ï¼ŒåŒ…å«å…·ä½“çš„æ™¯ç‚¹åç§°ã€åœ°å€ã€æè¿°ç­‰
4. ç”¨ä¸­æ–‡å›ç­”
5. å¦‚æœæœ‰å¤šä¸ªç›¸å…³æ™¯ç‚¹ï¼Œå¯ä»¥æ¨èå¤šä¸ª
6. å¯¹äºè¡Œç¨‹è§„åˆ’ï¼Œå¯ä»¥æŒ‰å¤©æ•°å®‰æ’ï¼Œè€ƒè™‘æ™¯ç‚¹çš„åœ°ç†ä½ç½®å’Œç±»å‹

å›ç­”:
"""
        
        try:
            answer = model_manager.generate(
                prompt=prompt,
                max_tokens=1000,
                temperature=0.7
            )
            
            logger.info(f"LLMç”Ÿæˆå›ç­”æˆåŠŸï¼Œé•¿åº¦: {len(answer)} å­—ç¬¦")
            return answer
            
        except Exception as e:
            logger.error(f"LLMç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def ask(self, question: str) -> Dict[str, Any]:
        """
        é—®ç­”ä¸»æµç¨‹
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            é—®ç­”ç»“æœï¼ŒåŒ…å«æœç´¢ç»“æœå’Œç”Ÿæˆçš„å›ç­”
        """
        logger.info(f"æ”¶åˆ°é—®é¢˜: {question}")
        
        # 1. å‘é‡æœç´¢
        search_results = self.search_attractions(question)
        
        # 2. ç”Ÿæˆå›ç­”
        answer = self.generate_answer(question, search_results)
        
        # 3. è¿”å›ç»“æœ
        return {
            "question": question,
            "search_results": search_results,
            "answer": answer,
            "results_count": len(search_results)
        }


def main():
    """äº¤äº’å¼é—®ç­”"""
    print("ğŸ¯ æ™¯ç‚¹é—®ç­”ç³»ç»Ÿï¼ˆå¤šæ¨¡å‹æ”¯æŒç‰ˆï¼‰")
    print("=" * 50)
    
    try:
        # å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ¨¡å‹
        model_type = settings.llm_model_type if hasattr(settings, 'llm_model_type') else None
        qa_system = AttractionQASystem(model_type=model_type)
        
        # æ˜¾ç¤ºå½“å‰æ¨¡å‹ä¿¡æ¯
        model_info = qa_system.get_current_model_info()
        print(f"âœ… é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        print(f"ğŸ¤– å½“å‰æ¨¡å‹: {model_info['model_name']} ({model_info['model_type']})")
        print(f"ğŸ”§ å¯ç”¨æ¨¡å‹: {', '.join(qa_system.list_available_models())}")
        print()
        
        while True:
            user_input = input("ğŸ” è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ (è¾“å…¥ 'quit' é€€å‡º, 'models' æŸ¥çœ‹æ¨¡å‹, 'switch <model>' åˆ‡æ¢æ¨¡å‹): ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if user_input.lower() == 'models':
                model_info = qa_system.get_current_model_info()
                print(f"ğŸ¤– å½“å‰æ¨¡å‹: {model_info['model_name']} ({model_info['model_type']})")
                print(f"ğŸ”§ å¯ç”¨æ¨¡å‹: {', '.join(qa_system.list_available_models())}")
                continue
            
            if user_input.lower().startswith('switch '):
                model_name = user_input[7:].strip()
                if qa_system.switch_model(model_name):
                    model_info = qa_system.get_current_model_info()
                    print(f"âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹: {model_info['model_name']}")
                else:
                    print(f"âŒ åˆ‡æ¢å¤±è´¥ï¼Œæ¨¡å‹ {model_name} ä¸å¯ç”¨")
                continue
            
            if not user_input:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜")
                continue
            
            print("\n" + "=" * 50)
            result = qa_system.ask(user_input)
            
            print(f"ğŸ” æœç´¢åˆ° {result['results_count']} ä¸ªç›¸å…³ç»“æœ")
            print(f"\nğŸ¤– AIå›ç­”:\n{result['answer']}")
            print("=" * 50 + "\n")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å†è§ï¼")
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")
        logger.error(f"ç³»ç»Ÿé”™è¯¯: {e}", exc_info=True)


if __name__ == "__main__":
    main()
